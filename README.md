# Deploying Vision Models (TensorFlow) from ðŸ¤— Transformers

_By [Chansung Park](https://github.com/deep-diver) and [Sayak Paul](https://github.com/sayakpaul)_

This repository shows various ways of deploying a vision model (TensorFlow) from ðŸ¤— Transformers using the TensorFlow Ecosystem. In particular, we use TensorFlow Serving (for local deployment), Vertex AI (serveless deployment), Kubernetes and GKE (more controlled deployment).

## Methods covered

- [x] Local TensorFlow Serving | [Blog post from ðŸ¤—](https://huggingface.co/blog/tf-serving-vision)
- [x] [Vertex AI Prediction](https://cloud.google.com/vertex-ai/docs/predictions/getting-predictions) 
- [ ] Vertex AI Prediction (w [optimized TFRT](https://cloud.google.com/vertex-ai/docs/predictions/optimized-tensorflow-runtime))
- [x] Kubernetes with GKE | [Blog post from ðŸ¤—](https://huggingface.co/blog/deploy-tfserving-kubernetes)

Know more about the optimized TFRT(TensorFlow RunTime) [here](https://github.com/tensorflow/runtime) and GKE(Google Kubernetes Engine) [here](https://cloud.google.com/kubernetes-engine).

## Each methods in detail

### TensorFlow Serving 
### Vertex AI Prediction
### Vertex AI Prediction (w/ optimized TFRT)
### Kubernetes with GKE(Google Kubernetes Engine)
